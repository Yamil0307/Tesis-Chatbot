import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.tools import tool
from typing import List, Dict, Any, TypedDict
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage

# --- IMPORTAR GESTORES CENTRALIZADOS ---
from rag_manager import get_rag_manager
from metadata_handler import MetadataHandler
from memory_manager import get_memory_manager

# --- 1. CONFIGURACI√ìN INICIAL ---
load_dotenv()

# NO inicializar aqu√≠ - se inicializar√° cuando se necesite
# rag_mgr se obtiene dentro de las funciones que lo necesitan

# El modelo LLM (Gemini para desarrollo)
# **ETAPA 2 - CORRECCI√ìN AGRESIVA: Temperatura 0 para NO generar conocimiento externo**
# Temperature=0 + strict system prompt = CERO alucinaciones
# El modelo ser√° 100% determinista y NUNCA usar√° conocimiento externo
llm = ChatGoogleGenerativeAI(model="gemini-robotics-er-1.5-preview", temperature=0.0)

# --- 2. DEFINICI√ìN DE LA HERRAMIENTA ---

@tool
def search_university_history(query: str) -> str:
    """Busca informaci√≥n √öNICAMENTE en los documentos hist√≥ricos y acad√©micos 
    de la Universidad de Oriente. No busca en fuentes externas.
    
    **ETAPA 2 - RESTRICCI√ìN: Solo conocimiento local de la universidad**
    
    Esta herramienta est√° limitada a:
    - Documentos hist√≥ricos de la Universidad de Oriente
    - Reglamentos acad√©micos
    - Estatutos y normativas
    - Archivos de la Sala de Fondos Raros y Valiosos
    
    Retorna:
    - Contexto: Fragmentos relevantes encontrados en los documentos locales
    - Fuentes: Lista de documentos consultados con p√°ginas
    
    Si no encuentra informaci√≥n relevante, lo reporta expl√≠citamente."""
    
    # Obtener el RAG Manager SOLO cuando se necesita (lazy initialization)
    rag_mgr = get_rag_manager()
    
    # Realiza la b√∫squeda usando el RAG Manager (SOLO en documentos locales)
    docs = rag_mgr.search(query, k=4)
    
    # Si no encuentra nada, devuelve un mensaje espec√≠fico
    if not docs:
        return "No se encontr√≥ informaci√≥n relevante en los documentos de la universidad."
    
    # Formatea el contexto CON anotaciones de fuente
    context = rag_mgr.format_context(docs)
    
    # **NUEVO: Extraer lista de fuentes para que el LLM las cite**
    sources_list = MetadataHandler.format_source_list(docs)
    
    # Combinar contexto + fuentes en un formato que el LLM entienda
    full_context = f"{context}{sources_list}"
    
    return f"Contexto recuperado de la universidad:\n{full_context}"


# Agrupamos todas las herramientas disponibles (solo tenemos una por ahora)
tools = [search_university_history]

# --- 3. DEFINICI√ìN DEL ESTADO DEL AGENTE (LangGraph) ---
class AgentState(TypedDict):
    """Representa el estado de la conversaci√≥n para LangGraph."""
    input: str 
    chat_history: List[Any]
    intermediate_steps: List[Any]
    context: str # <--- Dejamos 'context' como parte del estado.

# --- 4. DEFINICI√ìN DE LOS NODOS DE LA GR√ÅFICA (Los cerebros) ---

# Nodo A: El Agente principal (pensar y decidir)
def run_agent(state: AgentState) -> Dict[str, Any]:
    """El nodo principal FUERZA b√∫squeda en base de datos local SIEMPRE.
    
    **ETAPA 2 - CORRECCI√ìN ULTRA-AGRESIVA: NUNCA responde sin buscar primero**
    Esto previene que el LLM use su conocimiento general (alucinaciones).
    """
    
    # **CR√çTICO**: Forzar b√∫squeda SIEMPRE en la base de datos local
    # No permitimos que el LLM decida si usar o no la herramienta
    # Siempre busca primero, luego responde basado SOLO en lo encontrado
    
    # Llamada OBLIGATORIA a la herramienta de b√∫squeda
    query = state["input"]
    rag_mgr = get_rag_manager()
    docs = rag_mgr.search(query, k=4)
    
    # Formatear contexto
    if not docs:
        context = "[SIN RESULTADOS] No se encontr√≥ informaci√≥n relevante en los documentos de la universidad."
    else:
        context = rag_mgr.format_context(docs)
        sources_list = MetadataHandler.format_source_list(docs)
        context = f"{context}{sources_list}"
    
    # Retornar el contexto encontrado para que el nodo respond lo use
    return {"context": context, "intermediate_steps": []}

# Nodo B: El Respondedor Final (Generaci√≥n Aumentada)
def generate_response(state: AgentState) -> Dict[str, Any]:
    """Genera la respuesta final usando el contexto recuperado (RAG).
    
    Si no hay contexto relevante, responde de forma elegante indicando que
    la informaci√≥n no est√° disponible en los registros de la universidad.
    **ETAPA 2 - CORRECCI√ìN ULTRA-AGRESIVA: Validaci√≥n de alucinaciones**
    """
    
    context = state["context"]
    input_message = state["input"]
    current_chat_history = state["chat_history"]  # ‚Üê Obtener historial actual
    
    # **NUEVO: Detectar si el contexto est√° vac√≠o o no es relevante**
    is_context_empty = (
        not context or 
        context.strip() == "" or 
        "[SIN RESULTADOS]" in context
    )
    
    # Construir el historial de conversaci√≥n para el LLM
    conversation_history = ""
    if current_chat_history:
        conversation_history = "\n--- HISTORIAL DE CONVERSACI√ìN ANTERIOR ---\n"
        for i, msg in enumerate(current_chat_history):
            role = "Usuario" if hasattr(msg, '__class__') and msg.__class__.__name__ == 'HumanMessage' else "Asistente"
            conversation_history += f"{role}: {msg.content}\n"
        conversation_history += "--- FIN DEL HISTORIAL ---\n\n"
    
    # **ETAPA 2 - CORRECCI√ìN ULTRA-AGRESIVA: Forzar respuesta SOLO basada en contexto**
    # No permitimos NINGUNA alucinaci√≥n. El LLM DEBE responder solo del contexto.
    
    # Detectar si hay contexto relevante
    has_context = context and "[SIN RESULTADOS]" not in context and len(context.strip()) > 100
    
    if not has_context:
        # SIN contexto relevante - NO permitir que el LLM responda
        # Forzar respuesta predefinida sin pasar por el LLM
        response_content = "No contamos con informaci√≥n sobre este tema en los registros hist√≥ricos de la Universidad de Oriente."
    else:
        # CON contexto relevante - Permitir que el LLM responda basado en documentos
        system_prompt = (
            "ERES UN ASISTENTE ACAD√âMICO ESPECIALIZADO EN LA UNIVERSIDAD DE ORIENTE.\n\n"
            "Tu √öNICA fuente de verdad es el CONTEXTO DE LOS DOCUMENTOS.\n"
            "Responde SOLO usando la informaci√≥n del contexto.\n"
            "NO INVENTES INFORMACI√ìN. NO USES CONOCIMIENTO GENERAL.\n\n"
            "Estructura de tu respuesta:\n"
            "1. Responde la pregunta con informaci√≥n del contexto\n"
            "2. Al FINAL, incluye:\n"
            "   FUENTES CONSULTADAS:\n"
            "   - [Nombre del Documento] (p√°gina X)\n\n"
            f"{conversation_history}"
            "--- CONTEXTO DE LOS DOCUMENTOS ---\n"
            f"{context}\n"
            "--- FIN DEL CONTEXTO ---\n"
        )
        
        # Pasar TODO el historial + nuevo input al LLM
        messages = current_chat_history + [HumanMessage(content=input_message)]
        response_chain = llm.bind(system=system_prompt)
        final_response = response_chain.invoke(messages)
        response_content = final_response.content.strip()

    # CR√çTICO: AGREGAR a chat_history, no reemplazar
    # Agregar el mensaje del usuario + la respuesta del asistente
    new_messages = [
        HumanMessage(content=input_message),
        AIMessage(content=response_content)
    ]
    updated_chat_history = current_chat_history + new_messages
    
    return {"chat_history": updated_chat_history}

# Nodo C: L√≥gica de Herramientas (ELIMINADO - ya no es necesario)
# **ETAPA 2 - CORRECCI√ìN: La b√∫squeda ahora ocurre en run_agent directamente**
# Ya no necesitamos un nodo separado para ejecutar herramientas

# --- 6. CONDICI√ìN DE RUTA (Simplificada - ya no hay decisi√≥n) ---
def should_continue(state: AgentState) -> str:
    """Ya no hay decisi√≥n: SIEMPRE se ejecut√≥ la b√∫squeda en run_agent.
    
    **ETAPA 2 - CORRECCI√ìN: Eliminamos la l√≥gica de decisi√≥n**
    Ahora siempre vamos directamente a 'respond' porque ya buscamos en run_agent.
    """
    # Ya ejecutamos b√∫squeda en run_agent, vamos directamente a responder
    return "respond"


# --- 7. CONSTRUCCI√ìN DE LA GR√ÅFICA (El Diagrama de Flujo) ---
workflow = StateGraph(AgentState)

# Agrega los Nodos
workflow.add_node("agent", run_agent)          # El agente SIEMPRE busca
workflow.add_node("respond", generate_response) # Genera la respuesta

# Configura la entrada (Siempre empezamos por el agente)
workflow.set_entry_point("agent")

# **ETAPA 2 - CORRECCI√ìN: Flujo simplificado (sin decisi√≥n, solo b√∫squeda + respuesta)**
# Define la ruta despu√©s de buscar (Siempre va a responder)
workflow.add_edge("agent", "respond")

# Define el final de la conversaci√≥n (Todo termina respondiendo)
workflow.add_edge("respond", END)

# Compila el flujo de trabajo CON SqliteSaver para persistencia
memory_mgr = get_memory_manager()
saver = memory_mgr.get_saver()
app = workflow.compile(checkpointer=saver)

# --- 8. FUNCI√ìN DE PRUEBA ---

# --- 8. FUNCI√ìN DE PRUEBA ---

def test_agent(prompt: str):
    """Funci√≥n simple para interactuar con el agente en consola."""
    print(f"\nüôã‚Äç‚ôÇÔ∏è Usuario: {prompt}")
    
    # El estado inicial debe incluir 'context' para evitar el KeyError
    initial_state = {
        "input": prompt, 
        "chat_history": [],
        "context": "" # <--- ¬°AQU√ç EST√Å LA SOLUCI√ìN CLAVE!
    }
    
    # Invoca el agente y recorre todos los nodos
    final_state = app.invoke(initial_state)
    
    # Imprime la respuesta final
    print(f"ü§ñ Agente: {final_state['chat_history'][-1].content}")
    print("-" * 50)


# --- 9. EJECUCI√ìN ---

if __name__ == "__main__":
    
    # Pregunta 1: Debe usar la herramienta (buscar en los documentos locales de la universidad)
    test_agent("¬øQui√©n fue el autor de la tesis sobre la Sala de Fondos Raros y Valiosos?")
    
    # Pregunta 2: No encontrar√° informaci√≥n en los documentos locales (respuesta elegante sin conocimiento externo)
    test_agent("¬øQu√© color tiene el sol?")